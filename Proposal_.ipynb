{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You have the choice of doing either (1) an AI solve a problem style project or (2) run a Special Topics class on a topic of your choice.  This repo is assuming you want to do (1).  If you want to do (2) you should fill out the Gradescope proposal for that instead of using this repo.\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like 8-Queens or a small Traveling Salesman Problem or similar\n",
    "- If its the kind of problem (e.g., RL) that interacts with a simulator or live task, then the problem will have a reasonably complex action space. For instance, a wupus world kind of thing with a 9x9 grid is definitely too small.  A simulated mountain car with a less complex 2-d road and simplified dynamics seems like a fairly low achievement level.  A more complex 3-d mountain car simulation with large extent and realistic dynamics, sure sounds great!\n",
    "- If its the kind of problem that uses a dataset, then the dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "- The project must include some elements we talked about in the course\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Zixian Cai\n",
    "- Vinuthna Hasthi\n",
    "- Zheren Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our project aims to develop a reinforcement learning application that focuses on navigating a 3 dimensional maze with drones, with the purpose of applying it in delivery services such as grocery shopping.\n",
    "\n",
    "The goal is to incorporate machine learning methods like reinforcement learning to optimize delivery routes while taking into factors like distance, obstacles, time efficiency etc. It will use a 3D simulation to do so. The reinforcement learning model will be trained using algorithms from existing libraries such as open-ai gym or unity.\n",
    "\n",
    "Efficacy and efficiency will be measured via the reward system that we create. This is in the hopes of our algorithm being dynamic, meaning we can have a reward system that reduces any state space into a problem our reinforcement learning based machine can solve. With this, we can tackle problems with great range, from delivery services or grocery shopping to emergency responders like firefighters or with police chases. We will use the data from the simulations to compare and contrast which route might be best, which first-responder should arrive first, etc. This will bridge the gap from our subjective idea of what is best to a somewhat objective one. Evaluation metrics will include delivery efficiency, success rate, and adaptability across different maze configurations. So, by addressing these we want to increase the efficiency of our algorithm while offering a greater convenience to customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "\n",
    "\n",
    "Since the Covid-19 outbreak, no-contact food deliveries have become much more accessible. Additionally, the advancement in technology in the present has led to a huge change in lifestyle choices in fields such as shopping, and delivery services. There is an increased demand for convenient and efficient delivery methods. This coupled with the increase in drone usage in multiple industries gave us the inspiration to explore the possibility of a drone delivery service. The drones hold the advantage of bypassing traffic and navigating through complex environments asking the ideal for effective delivery services.  However, since field experiments with actual drones are of high cost and pose safety risks, using reinforcement learning can be an effective alternative that provides insights through simulating the experiment, which could then be applied to concrete applications.\n",
    "\n",
    "\n",
    "Some studies focus on the effectiveness of reinforcement learning in training agents to navigate a complex maze-like environment. For example, Mirowski et al.(2017) <a name=\"Mirowski\"></a>[<sup>[1]</sup>](#Mirowskinote) explored reinforcement learning techniques in 3D maze navigation with noticeable success in reasonably complex environments. In addition, other researchers such as Levine et al.(2016) <a name=\"Levine\"></a>[<sup>[2]</sup>](#Levinenote) also explored the effectiveness of reinforcement learning in broader robotic manipulation or navigation tasks, demonstrating the versatility and efficiency of the techniques across various complex tasks. (such as coating hanging task with a robotic arm)\n",
    "\n",
    "Furthermore, in this website that explores deep reinforcement learning for maze solving<a name=\"Zafrany\"></a>[<sup>[3]</sup>](#Zafranynote), it is mentioned that ‘The agent is experimenting and exploiting past experiences (episodes) in order to achieve its goal. It may fail again and again, but hopefully, after lots of trial and error (rewards and penalties) it will arrive to the solution of the problem. The solution will be reached if the agent finds the optimal sequence of states in which the accumulated sum of rewards is maximal’, which is the ultimate goal of our project. It is essential to define a running sum of the entire algorithm and when that sum reaches its minimum value, is when the algorithm is at its’ optimal state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The primary objective of this project is to develop and implement an algorithm which facilitates the navigation of a drone through maze-like systems to tackle problems like grocery shopping, food delivery etc. We are hoping to come up with an algorithm which efficiently handles constraints like distance, cost and time efficiency hence implementing an optimal algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use a 3-D simulation environment that includes various problems a drone may face during the process of grocery shopping, including weather, physical obstacles, fuel, etc, in order to simulate a realistic shopping scenario.\n",
    "\n",
    "In addition, the simulated drone should have sensors including camera, GPS, battery level to successfully detect and maneuver through obstacles, and detours for optimal navigation.\n",
    " \n",
    "Also, the data of interaction between the drone(agent) and the environment would be generated, including possible collision, travel time, battery level, delivery success rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We will use machine learning to solve our “maze” by treating it like a game with constraints. This will involve using a reward system that benefits the agent in numerous ways, but always designating the “best in slot” decision to be the most rewarding. For example, let our state space be our real-life roads, people, and stores. \n",
    "\n",
    "Our algorithm will filter through the numerous paths and decisions that a person may take in their mission to buy groceries. If they stop at a nail salon, this “path” will be deducted points as we are wasting time and money. But say we had a to-do list, where painting our nails was rewarding to that agent. Our algorithm will respond in kind by assigning different values to each cosmetic store. However, for the task at hand, simply going to the grocery store and picking up groceries will net the greatest score in our reward system. We then return a list of the most rewarding paths that agents can take, efficiently solving this problem. We hope to extrapolate this idea to all other state spaces, like food delivery services where the mission or purpose is to simply deliver the food as safely and quickly as possible. \n",
    "\n",
    "We will be using a reduction on commonly known algorithms like Djikstra’s for shortest path and assign edge weights accordingly, allowing our ML algorithm to be simple and effective in terms of its simulation and optimization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "The evaluation metrics would be the compound of several different metrics. \n",
    "The efficiency of delivery: The time taken to deliver the grocery along with the cost of the grocery and fuel(battery).\n",
    "Safety: Number of collisions and grocery integrity(?) during the grocery delivery simulation.\n",
    "Success rate: The percentage of successful deliveries.\n",
    "Adaptation ability: How well does the drone perform consistently across different combinations of the maze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure all the data collected to run the simulations on is consented by the customers itself or to prevent unauthorized access. \n",
    "* Inform customers of the intentions behind data collection and its role in service development\n",
    "*  Since a drone is involved, foresee any potential risks that could arise beyond. Clearly acknowledge the limitations of the drone navigation system. * \n",
    "If data like customer addresses, purchasing trends etc. are used make sure to anonymize customer details to prevent misuse of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Meet at least once a week.\n",
    "*  Show up to group meetings, contributing as much as you can.\n",
    "*  Always keep due dates in mind to complete the tasks in a timely manner.\n",
    "*  Check group chat messages regularly to ensure we don’t miss anything.\n",
    "*  If a problem arises, bring up the matter promptly.\n",
    "* If you’re stuck on something, talk with the group about it and/or go to office hours.\n",
    "ours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/10 | 6 PM | Brainstorm ideas for project | Decide on which idea to go with, discuss datasets |\n",
    "| 5/14 | 6 PM | Complete proposal; Search for more datasets if needed | Think about how to start designing the drone, begin thinking about what approach to take |\n",
    "| 5/16 | TBD | Start writing up code for the drone navigation and brainstorm different constraints to consider | Start assigning roles so all of us can work independently as well as discuss feasibility of different parts of the code |\n",
    "| 5/27 | TBD | Start writing up code for the drone navigation and brainstorm different constraints to consider | Begin analysis, discussing conclusions that can be drawn from the algorithm and go about initializing the drone. Completing all sections that show results |\n",
    "| 6/10 | TBD | Complete analysis, results, conclusion, and discussion | Discuss finishing touches. Turn in Final Project & Group Project Surveys | |& Group Project Surveys|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Mirowskinote\"></a>1.[^](#Mirowski): Mirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., Denil, M., Goroshin, R., Sifre, L., Kavukcuoglu, K., Kumaran, D., & Hadsell, R. (2017, January 13). Learning to navigate in complex environments. arXiv.org. https://arxiv.org/abs/1611.03673 \n",
    "<br> \n",
    "<a name=\"Levinenote\"></a>2.[^](#Levine): Levine, S., Finn, C., Darrell, T., & Abbeel, P. (2016, April 19). End-to-end training of deep visuomotor policies. arXiv.org. https://arxiv.org/abs/1504.00702 <br> \n",
    "<a name=\"Zafranynote\"></a>3.[^](#Zafrany):Zafrany, S. (n.d.). Deep reinforcement learning for maze solving¶. qmaze. https://www.samyzaf.com/ML/rl/qmaze.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
